{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from codecs import open\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(doc_file):\n",
    "    docs = []\n",
    "    labels = []\n",
    "    with open(doc_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            text_data = words[3:]\n",
    "            #Filtering stop words and also words with length 2 or less\n",
    "            text_data = [words for words in text_data if words not in stop_words and len(words)>=3]\n",
    "            #print(text_data)\n",
    "            docs.append(text_data)\n",
    "            labels.append(words[1])\n",
    "    return docs, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, labels = read_documents(r'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data set into training and validation data\n",
    "train_docs=[]\n",
    "\n",
    "split_point = int(0.80*len(docs))\n",
    "train_docs = docs[:split_point]\n",
    "train_labels = labels[:split_point]  \n",
    "val_docs = docs[split_point:]\n",
    "val_labels = labels[split_point:]\n",
    "#print(train_docs, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeOfVocab(doc):\n",
    "    length=0\n",
    "    for line in doc:\n",
    "        length += len(line) \n",
    "    print(\"length-->\", length)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(freq, total_count, sizeOfVocab): \n",
    "    likelihood={}\n",
    "    #For smoothing.. using alpha = 1\n",
    "    alpha = 1\n",
    "    for word, count in freq.items():\n",
    "        likelihood[word] = (count + alpha)/(total_count + alpha*sizeOfVocab)\n",
    "        print(word, likelihood[word]) \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method would check whether the word present in one label also present in other label. \n",
    "# If not present than fill their likelihood using smoothing function\n",
    "def checkWords(likelihood_pos, likelihood_neg,total_pos_words,total_neg_words,length):\n",
    "    alpha = 1\n",
    "    #Check if words in pos labels are present in neg labels and if not then fill their likelihood using smoothing function\n",
    "    for key in likelihood_pos.keys(): \n",
    "        if not key in likelihood_neg: \n",
    "            likelihood_neg[key]= alpha/(total_neg_words + alpha*length)\n",
    "            #print(\"likelihood_neg\",key, likelihood_neg[key])\n",
    "    \n",
    "    #Check if words in neg labels are present in pos labels and if not then fill their likelihood using smoothing function\n",
    "    for key in likelihood_neg.keys(): \n",
    "        if not key in likelihood_pos: \n",
    "            likelihood_pos[key]= alpha/(total_neg_words + alpha*length)\n",
    "            #print(\"likelihood_pos\",key, likelihood_pos[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate posterior for the documents\n",
    "def calculatePosterior(likelihood_pos, likelihood_neg, prior_pior_pos, prior_pior_neg):\n",
    "    \n",
    "    sum_likehood_pos = 0\n",
    "    sum_likehood_neg = 0\n",
    "   \n",
    "    #Summing up the likelihood of all pos words\n",
    "    for word, prob in likelihood_pos.items():\n",
    "        sum_likehood_pos += np.log(prob)\n",
    "    \n",
    "    s_pos = prior_pior_pos*sum_likehood_pos\n",
    "    print(\"############# s_pos \",s_pos)\n",
    "    \n",
    "    #Summing up the likelihood of all neg words\n",
    "    for word, prob in likelihood_neg.items():\n",
    "        sum_likehood_neg += np.log(prob)\n",
    "    \n",
    "    s_neg = prior_pior_neg*sum_likehood_neg\n",
    "    print(\"############# s_neg \",s_neg)\n",
    "   \n",
    "    post_prob_pos = s_pos/(s_pos + s_neg)\n",
    "    post_prob_neg = s_neg/(s_pos + s_neg)\n",
    "    #print(\"\\n############# Posterior P(pos|a doc) and P(neg|a doc)\",post_prob_pos, post_prob_neg)\n",
    "        \n",
    "    return post_prob_pos, post_prob_neg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying new documents - \"doc\" with a label \"label-pos or neg\"\n",
    "#def score_doc_label(document, label, <SOMETHING>):\n",
    "def score_doc_label(doc, prior_pos, prior_neg, pos_likehood, neg_likehood):\n",
    "    prob=1\n",
    "    \n",
    "    list_of_words = ' '.join(doc).split()\n",
    "    print(\"\\nlist of words -> \",list_of_words)\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        if word in pos_likehood:              \n",
    "            print(\"\\n  word and its likelihood-\",word,pos_likehood[word])\n",
    "            prob*=pos_likehood[word]\n",
    "        else:\n",
    "            #Case where word is not present in training set/Vocabulary.. Do nothing\n",
    "            pass\n",
    "    final_pos_prob=prior_pos*prob\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        if word in neg_likehood:\n",
    "            print(\"\\n word and its likelihood-\",word,neg_likehood[word])\n",
    "            prob*=neg_likehood[word]\n",
    "        else:\n",
    "            #Case where word is not present in training set/Vocabulary.. Do nothing\n",
    "            pass\n",
    "    final_neg_prob=prior_neg*prob\n",
    "    \n",
    "    print(\"\\nfinal_pos_prob....\", final_pos_prob)\n",
    "    print(\"\\nfinal_neg_prob....\", final_neg_prob)\n",
    "    \n",
    "    return final_pos_prob, final_neg_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Estimate the prior P(spam) and P(not spam)\n",
    "def prior_prob_event(labels, event):\n",
    "    label_count = Counter(labels)\n",
    "    prior = label_count[event]/sum(label_count.values())\n",
    "    return prior\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(docs, labels):\n",
    "    # 1.2 Count how many times each word appears in both spams and not spams:\n",
    "    freq_pos = Counter()\n",
    "    freq_neg = Counter()\n",
    "    for line, label in zip(docs, labels):\n",
    "        if label == 'pos':\n",
    "            freq_pos.update(line)\n",
    "        else:\n",
    "            freq_neg.update(line)\n",
    "    return freq_pos, freq_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pos = prior_prob_event(labels, 'pos')\n",
    "prior_neg = prior_prob_event(labels, 'neg')\n",
    "#print('prior pos', prior_pos)\n",
    "#print('prior neg', prior_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb(docs, labels):\n",
    "\n",
    "    # 1.2 Count how many times each word appears in both spams and not spams:\n",
    "    freq_pos, freq_neg = word_freq(docs, labels)\n",
    "        \n",
    "    # 1.3 Count how many words in total for each class:\n",
    "    total_pos_words = sum(freq_pos.values())\n",
    "    total_neg_words = sum(freq_neg.values())\n",
    "    #print(\"Total pos & neg values\", total_pos_words,total_neg_words )\n",
    "    \n",
    "    # 1.4 Estimate the likelihood P(wordi| spam) and P(wordi| not spam) for all wi âˆˆ V:\n",
    "    length = sizeOfVocab(docs)\n",
    "    \n",
    "    pos_likehood = likelihood(freq=freq_pos, total_count= total_pos_words, sizeOfVocab=length)\n",
    "    neg_likehood = likelihood(freq=freq_neg, total_count= total_neg_words, sizeOfVocab=length)\n",
    "    #print('pos likehood', pos_likehood)\n",
    "\n",
    "    # Calculate likelihood for words present in one label but not in other\n",
    "    checkWords(freq_pos,freq_neg,total_pos_words,total_neg_words,length)\n",
    "    \n",
    "    #Step 2: Estimate the prior P(spam) and P(not spam)\n",
    "    prior_pos = prior_prob_event(labels, 'pos')\n",
    "    prior_neg = prior_prob_event(labels, 'neg') # not positive\n",
    "    #print(\"prior_pos,prior_neg\", prior_pos,prior_neg)\n",
    "    \n",
    "    # Calculate Posterior probability\n",
    "    s_pos,s_neg= calculatePosterior(pos_likehood,neg_likehood,prior_pior_pos,prior_pior_neg)\n",
    "    \n",
    "    # Sample document and label to classify\n",
    "    doc=[\"excellent read highly recommended\"]\n",
    "    label=\"pos\"\n",
    "\n",
    "    #classify new doc/line\n",
    "    final_pos_prob,final_neg_prob = score_doc_label(doc, prior_pos, prior_neg, pos_likehood, neg_likehood)\n",
    "    \n",
    "    if final_pos_prob > final_neg_prob:\n",
    "        print(\"\\nNew lines have positive label......\")\n",
    "    else:\n",
    "        print(\"\\nNew lines have negative label.......\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length--> 158\n",
      "anything 0.010526315789473684\n",
      "purchase 0.010526315789473684\n",
      "left 0.010526315789473684\n",
      "behind 0.010526315789473684\n",
      "series 0.010526315789473684\n",
      "excellent 0.010526315789473684\n",
      "read 0.010526315789473684\n",
      "books 0.010526315789473684\n",
      "great 0.015789473684210527\n",
      "close 0.010526315789473684\n",
      "bible 0.010526315789473684\n",
      "entire 0.010526315789473684\n",
      "set 0.010526315789473684\n",
      "amazon 0.010526315789473684\n",
      "shopping 0.010526315789473684\n",
      "site 0.010526315789473684\n",
      "ship 0.010526315789473684\n",
      "fast 0.010526315789473684\n",
      "would 0.010526315789473684\n",
      "recommend 0.010526315789473684\n",
      "christian 0.010526315789473684\n",
      "wanting 0.010526315789473684\n",
      "know 0.010526315789473684\n",
      "expect 0.010526315789473684\n",
      "return 0.010526315789473684\n",
      "christ 0.010526315789473684\n",
      "fiction 0.010526315789473684\n",
      "still 0.010526315789473684\n",
      "makes 0.010526315789473684\n",
      "good 0.010526315789473684\n",
      "point 0.010526315789473684\n",
      "bought 0.007042253521126761\n",
      "album 0.01056338028169014\n",
      "loved 0.007042253521126761\n",
      "title 0.007042253521126761\n",
      "song 0.014084507042253521\n",
      "great 0.007042253521126761\n",
      "bad 0.007042253521126761\n",
      "rest 0.01056338028169014\n",
      "right 0.007042253521126761\n",
      "well 0.007042253521126761\n",
      "songs 0.007042253521126761\n",
      "filler 0.007042253521126761\n",
      "n't 0.007042253521126761\n",
      "worth 0.007042253521126761\n",
      "money 0.007042253521126761\n",
      "paid 0.007042253521126761\n",
      "either 0.007042253521126761\n",
      "shameless 0.007042253521126761\n",
      "bubblegum 0.007042253521126761\n",
      "oversentimentalized 0.007042253521126761\n",
      "depressing 0.007042253521126761\n",
      "tripe 0.007042253521126761\n",
      "kenny 0.007042253521126761\n",
      "chesney 0.007042253521126761\n",
      "popular 0.007042253521126761\n",
      "artist 0.007042253521126761\n",
      "result 0.007042253521126761\n",
      "cookie 0.007042253521126761\n",
      "cutter 0.007042253521126761\n",
      "category 0.007042253521126761\n",
      "nashville 0.007042253521126761\n",
      "music 0.014084507042253521\n",
      "scene 0.007042253521126761\n",
      "gotta 0.007042253521126761\n",
      "pump 0.007042253521126761\n",
      "albums 0.007042253521126761\n",
      "record 0.007042253521126761\n",
      "company 0.007042253521126761\n",
      "keep 0.01056338028169014\n",
      "lining 0.007042253521126761\n",
      "pockets 0.007042253521126761\n",
      "suckers 0.007042253521126761\n",
      "buying 0.01056338028169014\n",
      "garbage 0.01056338028169014\n",
      "perpetuate 0.007042253521126761\n",
      "coming 0.007042253521126761\n",
      "town 0.007042253521126761\n",
      "'ll 0.007042253521126761\n",
      "get 0.01056338028169014\n",
      "soapbox 0.007042253521126761\n",
      "country 0.01056338028169014\n",
      "really 0.01056338028169014\n",
      "needs 0.007042253521126761\n",
      "back 0.007042253521126761\n",
      "roots 0.007042253521126761\n",
      "stop 0.007042253521126761\n",
      "pop 0.007042253521126761\n",
      "nonsense 0.007042253521126761\n",
      "considered 0.007042253521126761\n",
      "mainstream 0.007042253521126761\n",
      "two 0.007042253521126761\n",
      "different 0.007042253521126761\n",
      "things 0.007042253521126761\n",
      "misled 0.007042253521126761\n",
      "thought 0.007042253521126761\n",
      "entire 0.007042253521126761\n",
      "contains 0.007042253521126761\n",
      "one 0.01056338028169014\n",
      "introduced 0.007042253521126761\n",
      "many 0.007042253521126761\n",
      "ell 0.007042253521126761\n",
      "high 0.007042253521126761\n",
      "school 0.007042253521126761\n",
      "students 0.007042253521126761\n",
      "lois 0.007042253521126761\n",
      "lowery 0.007042253521126761\n",
      "depth 0.007042253521126761\n",
      "characters 0.007042253521126761\n",
      "brilliant 0.007042253521126761\n",
      "writer 0.007042253521126761\n",
      "capable 0.007042253521126761\n",
      "inspiring 0.007042253521126761\n",
      "fierce 0.007042253521126761\n",
      "passion 0.007042253521126761\n",
      "readers 0.007042253521126761\n",
      "encounter 0.007042253521126761\n",
      "shocking 0.007042253521126761\n",
      "details 0.007042253521126761\n",
      "utopian 0.007042253521126761\n",
      "worlds 0.007042253521126761\n",
      "anxious 0.007042253521126761\n",
      "read 0.007042253521126761\n",
      "companion 0.007042253521126761\n",
      "novel 0.007042253521126761\n",
      "planned 0.007042253521126761\n",
      "share 0.007042253521126761\n",
      "class 0.007042253521126761\n",
      "january 0.007042253521126761\n",
      "although 0.007042253521126761\n",
      "series 0.007042253521126761\n",
      "written 0.007042253521126761\n",
      "6th 0.007042253521126761\n",
      "graders 0.007042253521126761\n",
      "older 0.007042253521126761\n",
      "book 0.007042253521126761\n",
      "simplicity 0.007042253521126761\n",
      "message 0.007042253521126761\n",
      "language 0.007042253521126761\n",
      "writing 0.007042253521126761\n",
      "style 0.007042253521126761\n",
      "inspire 0.007042253521126761\n",
      "sadly 0.007042253521126761\n",
      "disappointed 0.007042253521126761\n",
      "############# s_pos  -56.305887412603425\n",
      "############# s_neg  -332.98378630490913\n",
      "\n",
      "list of words ->  ['excellent', 'read', 'highly', 'recommended']\n",
      "\n",
      "  word and its likelihood- excellent 0.010526315789473684\n",
      "\n",
      "  word and its likelihood- read 0.010526315789473684\n",
      "\n",
      " word and its likelihood- read 0.007042253521126761\n",
      "\n",
      "final_pos_prob.... 4.4321329639889195e-05\n",
      "\n",
      "final_neg_prob.... 4.681830595762943e-07\n",
      "\n",
      "New lines have positive label......\n"
     ]
    }
   ],
   "source": [
    "train_nb(train_docs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
